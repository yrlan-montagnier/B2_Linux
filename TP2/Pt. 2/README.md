# TP2 pt. 2 : Maintien en condition op√©rationnelle

# Sommaire

- [TP2 pt. 2 : Maintien en condition op√©rationnelle](#tp2-pt-2--maintien-en-condition-op√©rationnelle)
- [Sommaire](#sommaire)
- [0. Pr√©requis](#0-pr√©requis)
  - [Checklist](#checklist)
- [I. Monitoring](#i-monitoring)
  - [1. Le concept](#1-le-concept)
  - [2. Setup](#2-setup)
- [II. Backup](#ii-backup)
  - [1. Intwo bwo](#1-intwo-bwo)
  - [2. Partage NFS](#2-partage-nfs)
  - [3. Backup de fichiers](#3-backup-de-fichiers)
  - [4. Unit√© de service](#4-unit√©-de-service)
    - [A. Unit√© de service](#a-unit√©-de-service)
    - [B. Timer](#b-timer)
    - [C. Contexte](#c-contexte)
  - [5. Backup de base de donn√©es](#5-backup-de-base-de-donn√©es)
  - [6. Petit point sur la backup](#6-petit-point-sur-la-backup)
- [III. Reverse Proxy](#iii-reverse-proxy)
  - [1. Introooooo](#1-introooooo)
  - [2. Setup simple](#2-setup-simple)
  - [3. Bonus HTTPS](#3-bonus-https)
- [IV. Firewalling](#iv-firewalling)
  - [1. Pr√©sentation de la syntaxe](#1-pr√©sentation-de-la-syntaxe)
  - [2. Mise en place](#2-mise-en-place)
    - [A. Base de donn√©es](#a-base-de-donn√©es)
    - [B. Serveur Web](#b-serveur-web)
    - [C. Serveur de backup](#c-serveur-de-backup)
    - [D. Reverse Proxy](#d-reverse-proxy)
    - [E. Tableau r√©cap](#e-tableau-r√©cap)

## Checklist

A chaque machine d√©ploy√©e, vous **DEVREZ** v√©rifier la üìù**checklist**üìù :

- [x] IP locale, statique ou dynamique
- [x] hostname d√©fini
- [x] firewall actif, qui ne laisse passer que le strict n√©cessaire
- [x] SSH fonctionnel avec un √©change de cl√©
- [x] acc√®s Internet (une route par d√©faut, une carte NAT c'est tr√®s bien)
- [x] r√©solution de nom
  - r√©solution de noms publics, en ajoutant un DNS public √† la machine
  - r√©solution des noms du TP, √† l'aide du fichier `/etc/hosts`
- [ ] monitoring (oui, toutes les machines devront √™tre surveill√©es)

# I. Monitoring

On bouge pas pour le moment niveau machines :

| Machine         | IP            | Service                 | Port ouvert | IPs autoris√©es |
|-----------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux` | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`  | `10.102.1.12` | Serveur Base de Donn√©es | ?           | ?             |

## 1. Le concept

**Dans notre cas on va surveiller deux choses :**

- d'une part, les machines : ***monitoring syst√®me***. Par exemple :
  - remplissage disque/RAM
  - charge CPU/r√©seau
- d'autre part, nos applications : ***monitoring applicatif***. Ici :
  - serveur Web
  - base de donn√©es

## 2. Setup


üåû **Setup Netdata**

- y'a plein de m√©thodes d'install pour Netdata
- on va aller au plus simple, ex√©cutez, sur toutes les machines que vous souhaitez monitorer :

```bash
# Passez en root pour cette op√©ration
$ sudo su -

# Install de Netdata via le script officiel statique
$ bash <(curl -Ss https://my-netdata.io/kickstart-static64.sh)

# Quittez la session de root
$ exit
```

üåû **Manipulation du *service* Netdata**

- un *service* `netdata` a √©t√© cr√©√©
- d√©terminer s'il est actif, et s'il est param√©tr√© pour d√©marrer au boot de la machine
  - si ce n'est pas le cas, faites en sorte qu'il d√©marre au boot de la machine
    ```
    [yrlan@web ~]$ sudo systemctl is-active netdata
    active
    [yrlan@web ~]$ sudo systemctl is-enabled netdata
    enabled
    ```

- d√©terminer √† l'aide d'une commande `ss` sur quel port Netdata √©coute
    - autoriser ce port dans le firewall
    ```bash
    [yrlan@web ~]$ sudo ss -alnpt | grep netdata
    LISTEN 0      128        127.0.0.1:8125       0.0.0.0:*    users:(("netdata",pid=2305,fd=45))
    LISTEN 0      128          0.0.0.0:19999      0.0.0.0:*    users:(("netdata",pid=2305,fd=5))
    LISTEN 0      128            [::1]:8125          [::]:*    users:(("netdata",pid=2305,fd=44))
    LISTEN 0      128             [::]:19999         [::]:*    users:(("netdata",pid=2305,fd=6))

    [yrlan@web ~]$ sudo firewall-cmd --add-port=19999/tcp --permanent; sudo firewall-cmd --add-port=8125/tcp --permanent
    success
    success
    [yrlan@web ~]$ sudo firewall-cmd --reload; sudo firewall-cmd --list-all
    success
    public (active)
      target: default
      icmp-block-inversion: no
      interfaces: enp0s3 enp0s8
      sources:
      services: ssh
      ports: 80/tcp 19999/tcp 8125/tcp
      protocols:
      masquerade: no
      forward-ports:
      source-ports:
      icmp-blocks:
      rich rules:
      
    PS C:\Users\yrlan> curl http://web.tp2.linux:19999/
    <!doctype html><html lang="en"><head><title>netdata dashboard</title>[...]</body></html>
    ```

#### **üåû Setup Alerting**

- **ajustez la conf de Netdata pour mettre en place des alertes Discord**
  - **ui ui c'est bien √ßa : vous recevrez un message Discord quand un seul critique est atteint**
  - **noubliez pas que la conf se trouve pour nous dans `/opt/netdata/etc/netdata/`**
    ```
    [yrlan@web ~]$ sudo cat /opt/netdata/etc/netdata/health_alarm_notify.conf
    ###############################################################################
    # sending discord notifications

    # note: multiple recipients can be given like this:
    #                  "CHANNEL1 CHANNEL2 ..."

    # enable/disable sending discord notifications
    SEND_DISCORD="YES"

    # Create a webhook by following the official documentation -
    # https://support.discordapp.com/hc/en-us/articles/228383668-Intro-to-Webhooks
    DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/897110887889006612/v-wKtDiq2QJYE6M1WMCPndNokPf6fbP-Ei33eGfhZ_DfnWXi0BfgDow4DQGfanYbAff2"

    # if a role's recipients are not configured, a notification will be send to
    # this discord channel (empty = do not send a notification for unconfigured
    # roles):
    DEFAULT_RECIPIENT_DISCORD="alarms"
    ``` 

- v√©rifiez le bon fonctionnement de l'alerting sur Discord
```
[yrlan@web ~]$ sudo su -s /bin/bash netdata
bash-4.4$ export NETDATA_ALARM_NOTIFY_DEBUG=1
bash-4.4$ /opt/netdata/usr/libexec/netdata/plugins.d/alarm-notify.sh test
[...]
--- END curl command ---
--- BEGIN received response ---
ok
--- END received response ---
RECEIVED HTTP RESPONSE CODE: 200
2021-10-12 01:32:50: alarm-notify.sh: INFO: sent discord notification for: web.tp2.linux test.chart.test_alarm is CLEAR to 'alarms'
# OK
```

# II. Backup

| Machine            | IP            | Service                 | Port ouvert | IPs autoris√©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de Donn√©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |

üñ•Ô∏è **VM `backup.tp2.linux`**

**D√©roulez la [üìù**checklist**üìù](#checklist) sur cette VM.**

## 2. Partage NFS

#### üåû **Setup environnement**

- **Cr√©er un dossier `/srv/backup/`**
- **Il contiendra un sous-dossier ppour chaque machine du parc**
    - **Commencez donc par cr√©er le dossier `/srv/backup/web.tp2.linux/`**
    ```
    [yrlan@backup ~]$ sudo mkdir -p /srv/backup/web.tp2.linux/
    ```
    
- **Il existera un partage NFS pour chaque machine (principe du moindre privil√®ge)**


#### **üåû Setup partage NFS**

- **Je crois que vous commencez √† conna√Ætre la chanson... Google "nfs server rocky linux"**
  - [ce lien me semble √™tre particuli√®rement simple et concis](https://www.server-world.info/en/note?os=Rocky_Linux_8&p=nfs&f=1)
```
[yrlan@backup ~]$ sudo dnf install -y nfs-utils
[yrlan@backup ~]$ sudo vi /etc/idmapd.conf
[yrlan@backup ~]$ sudo cat /etc/idmapd.conf | grep Domain
Domain = tp2.linux

[yrlan@backup ~]$ sudo vi /etc/exports
[yrlan@backup ~]$ sudo cat /etc/exports
/srv/backup/web.tp2.linux 10.102.1.11/24(rw,no_root_squash)

[yrlan@backup backup]$ sudo firewall-cmd --add-service=nfs --permanent; sudo firewall-cmd --reload; sudo firewall-cmd --list-all
success
success
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp0s3 enp0s8
  sources:
  services: nfs ssh
  ports:
  protocols:
  masquerade: no
  forward-ports:
  source-ports:
  icmp-blocks:
  rich rules:
  
[yrlan@backup backup]$ sudo systemctl enable --now rpcbind nfs-server
```

#### **üåû Setup points de montage sur `web.tp2.linux`**

- [sur le m√™me site, y'a √ßa](https://www.server-world.info/en/note?os=Rocky_Linux_8&p=nfs&f=2)
- **Monter le dossier `/srv/backups/web.tp2.linux` du serveur NFS dans le dossier `/srv/backup/` du serveur Web**
```
[yrlan@web ~]$ sudo dnf -y install nfs-utils
[yrlan@web ~]$ sudo cat /etc/idmapd.conf | grep Domain
Domain = tp2.linux
[yrlan@web ~]$ sudo mkdir /srv/backup
[yrlan@web ~]$ sudo mount -t nfs backup.tp2.linux:/srv/backup/web.tp2.linux /srv/backup
```

- **V√©rifier...**
    - **Avec une commande `mount` que la partition est bien mont√©e**
    ```
    [yrlan@web ~]$ sudo mount | grep backup
    backup.tp2.linux:/srv/backup/web.tp2.linux on /srv/backup type nfs4 (rw,relatime,vers=4.2,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.102.1.11,local_lock=none,addr=10.102.1.13)
    ```

    - Avec une commande `df -h` qu'il reste de la place
    ```
    [yrlan@web ~]$ sudo df -h | grep backup
    backup.tp2.linux:/srv/backup/web.tp2.linux  6.2G  2.2G  4.1G  35% /srv/backup
    ```

    - **Avec une commande `touch` que vous avez le droit d'√©crire dans cette partition**
    ```
    # Cr√©ation d'un fichier `testttt` dans /srv/backup
    [yrlan@web ~]$ sudo touch /srv/backup/testttt
    [yrlan@web ~]$ sudo ls -l /srv/backup/
    total 0
    -rw-r--r--. 1 root root 0 Oct 12 03:22 testttt
    
    # Il apparait bien dans le dossier /srv/backup/web.tp2.linux/ sur la machine backup
    [yrlan@backup ~]$ ls -l /srv/backup/web.tp2.linux/
    total 0
    -rw-r--r--. 1 root root 0 Oct 12 03:22 testttt
    ```

- **Faites en sorte que cette partition se monte automatiquement gr√¢ce au fichier `/etc/fstab`**
    ```
    [yrlan@web ~]$ sudo vi /etc/fstab
    [yrlan@web ~]$ sudo cat /etc/fstab | grep backup
    backup.tp2.linux:/srv/backup/web.tp2.linux /srv/backup nfs defaults 0 0
    
    ## TEST : 
    [yrlan@web ~]$ sudo umount /srv/backup
    [yrlan@web ~]$ sudo mount -av | grep /srv/backup
    /srv/backup              : successfully mounted
    ```
    
#### **üåü BONUS : partitionnement avec LVM**

- **Ajoutez un disque √† la VM `backup.tp2.linux` (disque = sdb)**
- **Utilisez LVM pour cr√©er une nouvelle partition (5Go √ßa ira)**
```bash
[yrlan@backup ~]$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0    8G  0 disk
‚îú‚îÄsda1        8:1    0    1G  0 part /boot
‚îî‚îÄsda2        8:2    0    7G  0 part
  ‚îú‚îÄrl-root 253:0    0  6.2G  0 lvm  /
  ‚îî‚îÄrl-swap 253:1    0  820M  0 lvm  [SWAP]
sdb           8:16   0    8G  0 disk
sr0          11:0    1 1024M  0 rom

## On cr√©e un Physical Volume sur le disque qu'on a rep√©r√©
[yrlan@backup ~]$ sudo pvcreate /dev/sdb; sudo pvs
  Physical volume "/dev/sdb" successfully created.
  
  PV         VG Fmt  Attr PSize  PFree
  /dev/sda2  rl lvm2 a--  <7.00g    0
  /dev/sdb      lvm2 ---   8.00g 8.00g  
    
# On cr√©e un Volume Group
[yrlan@backup ~]$ sudo vgcreate backup /dev/sdb; sudo vgs
  Volume group "backup" successfully created
  VG     #PV #LV #SN Attr   VSize  VFree
  backup   1   0   0 wz--n- <8.00g <8.00g
  rl       1   2   0 wz--n- <7.00g     0
    
# On cr√©e un Logical Volume ( Logical Volume = Partition )
[yrlan@backup ~]$ sudo lvcreate -L 5G backup -n Backup
  Logical volume "Backup" created.
[yrlan@backup ~]$ sudo lvs
  LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  Backup backup -wi-a-----   5.00g
  root   rl     -wi-ao----  <6.20g
  swap   rl     -wi-ao---- 820.00m

# Formater partition en ext4
[yrlan@backup ~]$ sudo mkfs -t ext4 /dev/mapper/backup-Backup
mke2fs 1.45.6 (20-Mar-2020)
Creating filesystem with 1310720 4k blocks and 327680 inodes
Filesystem UUID: 2c1c6c5b-62bf-4ee2-b96f-2750feaed879
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736

Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done

# Montage de la partition ( notre Lv ) sur /srv/backup
[yrlan@backup ~]$ sudo mount /dev/backup/Backup /srv/backup

# V√©rifs :
[yrlan@backup ~]$ df -h | grep backup
/dev/mapper/backup-Backup  4.9G   20M  4.6G   1% /srv/backup

[yrlan@backup ~]$ lsblk | grep backup
‚îî‚îÄbackup-Backup 253:2    0    5G  0 lvm  /srv/backup
```

- **Monter automatiquement cette partition au d√©marrage du syst√®me √† l'aide du fichier `/etc/fstab`**
- **Cette nouvelle partition devra √™tre mont√©e sur le dossier `/srv/backup/`**

```bash
# Config
[yrlan@backup ~]$ sudo cat /etc/fstab | grep /dev/backup/Backup
/dev/backup/Backup /srv/backup ext4 defaults 0 0

# V√©rif montage auto partition sur /srv/backup
[yrlan@backup ~]$ sudo umount /srv/backup
[yrlan@backup ~]$ sudo mount -av | grep /srv/backup
/srv/backup              : successfully mounted

# V√©rif montage sur machine Web
[yrlan@web ~]$ sudo umount /srv/backup
[yrlan@web ~]$ sudo mount -av | grep /srv/backup
/srv/backup              : successfully mounted

# C'est bien notre partition de 5Go qui est utilis√© pour le dossier /srv/backup
[yrlan@web ~]$ df -h | grep backup
backup.tp2.linux:/srv/backup/web.tp2.linux  4.9G   20M  4.6G   1% /srv/backup

[yrlan@db ~]$ df -h | grep backup
backup.tp2.linux:/srv/backup/db.tp2.linux  4.9G   20M  4.6G   1% /srv/backup

[yrlan@backup ~]$ df -h | grep backup
/dev/mapper/backup-Backup  4.9G   20M  4.6G   1% /srv/backup

[yrlan@backup ~]$ lsblk | grep backup
‚îî‚îÄbackup-Backup 253:2    0    5G  0 lvm  /srv/backup
```

## **3. Backup de fichiers**

**Un peu de scripting `bash` !** Le scripting est le meilleur ami de l'admin, vous allez pas y couper hihi.  

La syntaxe de `bash` est TRES particuli√®re, mais ce que je vous demande de r√©aliser l√† est un script minimaliste.

Votre script **DEVRA**...

- comporter un shebang
- comporter un commentaire en en-t√™te qui indique le but du script, en quelques mots
- comporter un commentaire qui indique l'auteur et la date d'√©criture du script

Par exemple :

```bash
#!/bin/bash
# Simple backup script
# it4 - 09/10/2021

...
```

üåû **R√©diger le script de backup `/srv/tp2_backup.sh`**

- le script cr√©e une archive compress√©e `.tar.gz` du dossier cibl√©
  - cela se fait avec la commande `tar`
- l'archive g√©n√©r√©e doit s'appeler `tp2_backup_YYMMDD_HHMMSS.tar.gz`
  - vous remplacerez √©videmment `YY` par l'ann√©e (`21`), `MM` par le mois (`10`), etc.
  - ces infos sont d√©termin√©es dynamiquement au moment o√π le script s'ex√©cute √† l'aide de la commande `date`
- le script utilise la commande `rsync` afin d'envoyer la sauvegarde dans le dossier de destination
- il **DOIT** pouvoir √™tre appel√© de la sorte :

```bash
$ ./tp2_backup.sh <DESTINATION> <DOSSIER_A_BACKUP>
```

üìÅ **Fichier `/srv/tp2_backup.sh`**

> **Il est strictement hors de question d'utiliser `sudo` dans le contenu d'un script.**  
Il est envisageable, en revanche, que le script doive √™tre lanc√© avec root ou la commande `sudo` afin d'obtenir des droits √©lev√©s pendant son ex√©cution.

üåû **Tester le bon fonctionnement**

- ex√©cuter le script sur le dossier de votre choix
- prouvez que la backup s'est bien ex√©cut√©e
- **tester de restaurer les donn√©es**
  - r√©cup√©rer l'archive g√©n√©r√©e, et v√©rifier son contenu

üåü **BONUS**

- faites en sorte que votre script ne conserve que les 5 backups les plus r√©centes apr√®s le `rsync`
- faites en sorte qu'on puisse passer autant de dossier qu'on veut au script : `./tp2_backup.sh <DESTINATION> <DOSSIER1> <DOSSIER2> <DOSSIER3>...` et n'obtenir qu'une seule archive
- utiliser [Borg](https://borgbackup.readthedocs.io/en/stable/) plut√¥t que `rsync`

## 4. Unit√© de service

Lancer le script √† la main c'est bien. **Le mettre dans une joulie *unit√© de service* et l'ex√©cuter √† intervalles r√©guliers, de mani√®re automatis√©e, c'est mieux.**

Le but va √™tre de cr√©er un *service* systemd pour que vous puissiez interagir avec votre script de sauvegarde en faisant :

```bash
$ sudo systemctl start tp2_backup
$ sudo systemctl status tp2_backup
```

Ensuite on cr√©era un *timer systemd* qui permettra de d√©clencher le lancement de ce *service* √† intervalles r√©guliers.

**La classe nan ?**

![systemd can do that](./pics/suprised-cat.jpg)

---

### A. Unit√© de service

üåû **Cr√©er une *unit√© de service*** pour notre backup

- c'est juste un fichier texte hein
- doit se trouver dans le dossier `/etc/systemd/system/`
- doit s'appeler `tp2_backup.service`
- le contenu :

```bash
[Unit]
Description=Our own lil backup service (TP2)

[Service]
ExecStart=/srv/tp2_backup.sh <DESTINATION> <DOSSIER>
Type=oneshot
RemainAfterExit=no

[Install]
WantedBy=multi-user.target
```

> Pour les tests, sauvegardez le dossier de votre choix, peu importe lequel.

üåû **Tester le bon fonctionnement**

- n'oubliez pas d'ex√©cuter `sudo systemctl daemon-reload` √† chaque ajout/modification d'un *service*
- essayez d'effectuer une sauvegarde avec `sudo systemctl start backup`
- prouvez que la backup s'est bien ex√©cut√©e
  - v√©rifiez la pr√©sence de la nouvelle archive

---

### B. Timer

Un *timer systemd* permet l'ex√©cution d'un *service* √† intervalles r√©guliers.

üåû **Cr√©er le *timer* associ√© √† notre `tp2_backup.service`**

- toujours juste un fichier texte
- dans le dossier `/etc/systemd/system/` aussi
- fichier `tp2_backup.timer`
- contenu du fichier :

```bash
[Unit]
Description=Periodically run our TP2 backup script
Requires=tp2_backup.service

[Timer]
Unit=tp2_backup.service
OnCalendar=*-*-* *:*:00

[Install]
WantedBy=timers.target
```

> Le nom du *timer* doit √™tre rigoureusement identique √† celui du *service*. Seule l'extension change : de `.service` √† `.timer`. C'est notamment gr√¢ce au nom identique que systemd sait que ce *timer* correspond √† un *service* pr√©cis.

üåû **Activez le timer**

- d√©marrer le *timer* : `sudo systemctl start tp2_backup.timer`
- activer le au d√©marrage avec une autre commande `systemctl`
- prouver que...
  - le *timer* est actif actuellement
  - qu'il est param√©tr√© pour √™tre actif d√®s que le syst√®me boot

üåû **Tests !**

- avec la ligne `OnCalendar=*-*-* *:*:00`, le *timer* d√©clenche l'ex√©cution du *service* toutes les minutes
- v√©rifiez que la backup s'ex√©cute correctement

---

### C. Contexte

üåû **Faites en sorte que...**

- votre backup s'ex√©cute sur la machine `web.tp2.linux`
- le dossier sauvegard√© est celui qui contient le site NextCloud (quelque part dans `/var/`)
- la destination est le dossier NFS mont√© depuis le serveur `backup.tp2.linux`
- la sauvegarde s'ex√©cute tous les jours √† 03h15 du matin
- prouvez avec la commande `sudo systemctl list-timers` que votre *service* va bien s'ex√©cuter la prochaine fois qu'il sera 03h15

üìÅ **Fichier `/etc/systemd/system/tp2_backup.timer`**  
üìÅ **Fichier `/etc/systemd/system/tp2_backup.service`**

## 5. Backup de base de donn√©es

Sauvegarder des dossiers c'est bien. Mais sauvegarder aussi les bases de donn√©es c'est mieux.

üåû **Cr√©ation d'un script `/srv/tp2_backup_db.sh`**

- il utilise la commande `mysqldump` pour r√©cup√©rer les donn√©es de la base de donn√©es
- cela g√©n√®re un fichier `.sql` qui doit ensuite √™tre compress√© en `.tar.gz`
- il s'ex√©cute sur la machine `db.tp2.linux`
- il s'utilise de la fa√ßon suivante :

```bash
$ ./tp2_backup_db.sh <DESTINATION> <DATABASE>
```

üìÅ **Fichier `/srv/tp2_backup_db.sh`**  

üåû **Restauration**

- tester la restauration de donn√©es
- c'est √† dire, une fois la sauvegarde effectu√©e, et le `tar.gz` en votre possession, tester que vous √™tes capables de restaurer la base dans l'√©tat au moment de la sauvegarde
  - il faut r√©injecter le fichier `.sql` dans la base √† l'aide d'une commmande `mysql`

üåû ***Unit√© de service***

- pareil que pour la sauvegarde des fichiers ! On va faire de ce script une *unit√© de service*.
- votre script `/srv/tp2_backup_db.sh` doit pouvoir se lancer gr√¢ce √† un *service* `tp2_backup_db.service`
- le *service* est ex√©cut√© tous les jours √† 03h30 gr√¢ce au *timer* `tp2_backup_db.timer`
- prouvez le bon fonctionnement du *service* ET du *timer*

üìÅ **Fichier `/etc/systemd/system/tp2_backup_db.timer`**  
üìÅ **Fichier `/etc/systemd/system/tp2_backup_db.service`**

## 6. Petit point sur la backup

A ce stade vous avez :

- un script qui tourne sur `web.tp2.linux` et qui **sauvegarde les fichiers de NextCloud**
- un script qui tourne sur `db.tp2.linux` et qui **sauvegarde la base de donn√©es de NextCloud**
- toutes **les backups sont centralis√©es** sur `backup.tp2.linux`
- **tout est g√©r√© de fa√ßon automatis√©e**
  - les scripts sont packag√©s dans des *services*
  - les services sont d√©clench√©s par des *timers*
  - tout est param√©tr√© pour s'allumer quand les machines boot (les *timers* comme le serveur NFS)

üî•üî• **That is clean shit.** üî•üî•

# III. Reverse Proxy

## 1. Introooooo

Un *reverse proxy* est un outil qui sert d'interm√©diaire entre le client et un serveur donn√© (souvent un serveur Web).

**C'est l'admin qui le met en place, afin de prot√©ger l'acc√®s au serveur Web.**

Une fois en place, le client devra saisir l'IP (ou le nom) du *reverse proxy* pour acc√©der √† l'application Web (ce ne sera plus directement l'IP du serveur Web).

Un *reverse proxy* peut permettre plusieurs choses :

- chiffrement
  - c'est lui qui mettra le HTTPS en place (protocole HTTP + chiffrement avec le protocole TLS)
  - on pourrait le faire directement avec le serveur Web (Apache) dans notre cas
  - pour de meilleures performances, il est pr√©f√©rable de d√©dier une machine au chiffrement HTTPS, et de laisser au serveur web un unique job : traiter les requ√™tes HTTP
- r√©partition de charge
  - plut√¥t qu'avoir un seul serveur Web, on peut en setup plusieurs
  - ils h√©bergent tous la m√™me application
  - le *reverse proxy* enverra les clients sur l'un ou l'autre des serveurs Web, afin de r√©partir la charge √† traiter
- d'autres trucs
  - caching de ressources statiques (CSS, JSS, images, etc.)
  - tol√©rance de pannes
  - ...

---

**Dans ce TP on va setup un reverse proxy NGINX tr√®s simpliste.**

![Apache at the back hihi](./pics/nginx-at-the-front-apache-at-the-back.jpg)

## 2. Setup simple

| Machine            | IP            | Service                 | Port ouvert | IPs autoris√©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de Donn√©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |
| `front.tp2.linux`  | `10.102.1.14` | Reverse Proxy           | ?           | ?             |

üñ•Ô∏è **VM `front.tp2.linu`x**

**D√©roulez la [üìù**checklist**üìù](#checklist) sur cette VM.**

üåû **Installer NGINX**

- vous devrez d'abord installer le paquet `epel-release` avant d'installer `nginx`
  - EPEL c'est des d√©p√¥ts additionnels pour Rocky
  - NGINX n'est pas pr√©sent dans les d√©p√¥ts par d√©faut que conna√Æt Rocky
- le fichier de conf principal de NGINX est `/etc/nginx/nginx.conf`

üåû **Tester !**

- lancer le *service* `nginx`
- le param√©trer pour qu'il d√©marre seul quand le syst√®me boot
- rep√©rer le port qu'utilise NGINX par d√©faut, pour l'ouvrir dans le firewall
- v√©rifier que vous pouvez joindre NGINX avec une commande `curl` depuis votre PC

üåû **Explorer la conf par d√©faut de NGINX**

- rep√©rez l'utilisateur qu'utilise NGINX par d√©faut
- dans la conf NGINX, on utilise le mot-cl√© `server` pour ajouter un nouveau site
  - rep√©rez le bloc `server {}` dans le fichier de conf principal
- par d√©faut, le fichier de conf principal inclut d'autres fichiers de conf
  - mettez en √©vidence ces lignes d'inclusion dans le fichier de conf principal

üåû **Modifier la conf de NGINX**

- pour que √ßa fonctionne, le fichier `/etc/hosts` de la machine **DOIT** √™tre rempli correctement, conform√©ment √† la **[üìù**checklist**üìù](#checklist)**
- supprimer le bloc `server {}` par d√©faut, pour ne plus pr√©senter la page d'accueil NGINX
- cr√©er un fichier `/etc/nginx/conf.d/web.tp2.linux.conf` avec le contenu suivant :
  - j'ai sur-comment√© pour vous expliquer les lignes, n'h√©sitez pas √† d√©gommer mes lignes de commentaires

```bash
[it4@localhost nginx]$ cat conf.d/web.tp2.linux.conf 
server {
    # on demande √† NGINX d'√©couter sur le port 80 pour notre NextCloud
    listen 80;

    # ici, c'est le nom de domaine utilis√© pour joindre l'application
    # ce n'est pas le nom du reverse proxy, mais le nom que les clients devront saisir pour atteindre le site
    server_name web.tp2.linux; # ici, c'est le nom de domaine utilis√© pour joindre l'application (pas forc√©me

    # on d√©finit un comportement quand la personne visite la racine du site (http://web.tp2.linux/)
    location / {
        # on renvoie tout le trafic vers la machine web.tp2.linux
        proxy_pass http://web.tp2.linux;
    }
}
```

## 3. Bonus HTTPS

**Etape bonus** : mettre en place du chiffrement pour que nos clients acc√®dent au site de fa√ßon plus s√©curis√©e.

üåü **G√©n√©rer la cl√© et le certificat pour le chiffrement**

- il existe plein de fa√ßons de faire
- nous allons g√©n√©rer en une commande la cl√© et le certificat
- puis placer la cl√© et le cert dans les endroits standards pour la distribution Rocky Linux

```bash
# On se d√©place dans un dossier o√π on peut √©crire
$ cd ~

# G√©n√©ration de la cl√© et du certificat
# Attention √† bien saisir le nom du site pour le "Common Name"
$ openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout server.key -out server.crt
[...]
Common Name (eg, your name or your server\'s hostname) []:web.tp2.linux
[...]

# On d√©place la cl√© et le certificat dans les dossiers standards sur Rocky
# En le renommant
$ sudo mv server.key /etc/pki/tls/private/web.tp2.linux.key
$ sudo mv server.crt /etc/pki/tls/certs/web.tp2.linux.crt

# Setup des permissions restrictives
$ sudo chown root:root /etc/pki/tls/private/web.tp2.linux.key
$ sudo chown root:root /etc/pki/tls/certs/web.tp2.linux.crt
$ sudo chmod 400 /etc/pki/tls/private/web.tp2.linux.key
$ sudo chmod 644 /etc/pki/tls/certs/web.tp2.linux.crt
```

üåü **Modifier la conf de NGINX**

- inspirez-vous de ce que vous trouvez sur internet
- il n'y a que deux lignes √† ajouter
  - une ligne pour pr√©ciser le chemin du certificat
  - une ligne pour pr√©ciser le chemin de la cl√©
- et une ligne √† modifier
  - pr√©ciser qu'on √©coute sur le port 443, avec du chiffrement
- n'oubliez pas d'ouvrir le port 443/tcp dans le firewall

üåü **TEST**

- connectez-vous sur `https://web.tp2.linux` depuis votre PC
- petite avertissement de s√©cu : normal, on a sign√© nous-m√™mes le certificat
  - vous pouvez donc "Accepter le risque" (le nom du bouton va changer suivant votre navigateur)
  - avec `curl` il faut ajouter l'option `-k` pour d√©sactiver cette v√©rification

# IV. Firewalling

**On va rendre nos firewalls un peu plus agressifs.**

Actuellement je vous ai juste demand√© d'autoriser le trafic sur tel ou tel port. C'est bien.

**Maintenant on va restreindre le trafic niveau IP aussi.**

Par exemple : notre base de donn√©es `db.tp2.linux` n'est acc√©d√©e que par le serveur Web `web.tp2.linux`, et par aucune autre machine.  
On va donc configurer le firewall de la base de donn√©es pour qu'elle n'accepte QUE le trafic qui vient du serveur Web.

**On va *harden* ("durcir" en fran√ßais) la configuration de nos firewalls.**

## 1. Pr√©sentation de la syntaxe

> **N'oubliez pas d'ajouter `--permanent` sur toutes les commandes `firewall-cmd`** si vous souhaitez que le changement reste effectif apr√®s un rechargement de FirewallD.

**Premi√®re √©tape** : d√©finir comme politique par d√©faut de TOUT DROP. On refuse tout, et on whiteliste apr√®s.

Il existe d√©j√† une zone appel√©e `drop` qui permet de jeter tous les paquets. Il suffit d'ajouter nos interfaces dans cette zone.

```bash
$ sudo firewall-cmd --list-all # on voit qu'on est par d√©faut dans la zone "public"
$ sudo firewall-cmd --set-default-zone=drop # on configure la zone "drop" comme zone par d√©faut
$ sudo firewall-cmd --zone=drop --add-interface=enp0s8 # ajout explicite de l'interface host-only √† la zone "drop"
```

**Ensuite**, on peut cr√©er une nouvelle zone, qui autorisera le trafic li√© √† telle ou telle IP source :

```bash
$ sudo firewall-cmd --add-zone=ssh # le nom "ssh" est compl√®tement arbitraire. C'est clean de faire une zone par service.
```

**Puis** on d√©finit les r√®gles visant √† autoriser un trafic donn√© :

```bash
$ sudo firewall-cmd --zone=ssh --add-source=10.102.1.1/32 # 10.102.1.1 sera l'IP autoris√©e
$ sudo firewall-cmd --zone=ssh --add-port=22/tcp # uniquement le trafic qui vient 10.102.1.1, √† destination du port 22/tcp, sera autoris√©
```

**Le comportement de FirewallD sera alors le suivant :**

- si l'IP source d'un paquet est `10.102.1.1`, il traitera le paquet comme √©tant dans la zone `ssh`
- si l'IP source est une autre IP, et que le paquet arrive par l'interface `enp0s8` alors le paquet sera g√©r√© par la zone `drop` (le paquet sera donc *dropped* et ne sera jamais trait√©)

> *L'utilisation de la notation `IP/32` permet de cibler une IP sp√©cifique. Si on met le vrai masque `10.102.1.1/24` par exemple, on autorise TOUT le r√©seau `10.102.1.0/24`, et non pas un seul h√¥te. Ce `/32` c'est un truc qu'on voit souvent en r√©seau, pour faire r√©f√©rence √† une IP unique.*

![Cut here to activate firewall :D](./pics/cut-here-to-activate-firewall-best-label-for-lan-cable.jpg)

## 2. Mise en place

### A. Base de donn√©es

üåû **Restreindre l'acc√®s √† la base de donn√©es `db.tp2.linux`**

- seul le serveur Web doit pouvoir joindre la base de donn√©es sur le port 3306/tcp
- vous devez aussi autoriser votre acc√®s SSH
- n'h√©sitez pas √† multiplier les zones (une zone `ssh` et une zone `db` par exemple)

> Quand vous faites une connexion SSH, vous la faites sur l'interface Host-Only des VMs. Cette interface est branch√©e √† un Switch qui porte le nom du Host-Only. Pour rappel, votre PC a aussi une interface branch√©e √† ce Switch Host-Only.  
C'est depuis cette IP que la VM voit votre connexion. C'est cette IP que vous devez autoriser dans le firewall de votre VM pour SSH.

üåû **Montrez le r√©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

- `sudo firewall-cmd --get-active-zones`
- `sudo firewall-cmd --get-default-zone`
- `sudo firewall-cmd --list-all --zone=?`

### B. Serveur Web

üåû **Restreindre l'acc√®s au serveur Web `web.tp2.linux`**

- seul le reverse proxy `front.tp2.linux` doit acc√©der au serveur web sur le port 80
- n'oubliez pas votre acc√®s SSH

üåû **Montrez le r√©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### C. Serveur de backup

üåû **Restreindre l'acc√®s au serveur de backup `backup.tp2.linux`**

- seules les machines qui effectuent des backups doivent √™tre autoris√©es √† contacter le serveur de backup *via* NFS
- n'oubliez pas votre acc√®s SSH

üåû **Montrez le r√©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### D. Reverse Proxy

üåû **Restreindre l'acc√®s au reverse proxy `front.tp2.linux`**

- seules les machines du r√©seau `10.102.1.0/24` doivent pouvoir joindre le proxy
- n'oubliez pas votre acc√®s SSH

üåû **Montrez le r√©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### E. Tableau r√©cap

üåû **Rendez-moi le tableau suivant, correctement rempli :**

| Machine            | IP            | Service                 | Port ouvert | IPs autoris√©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de Donn√©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |
| `front.tp2.linux`  | `10.102.1.14` | Reverse Proxy           | ?           | ?             |
