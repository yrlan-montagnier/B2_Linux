# TP2 pt. 2 : Maintien en condition opÃ©rationnelle

# Sommaire

- [TP2 pt. 2 : Maintien en condition opÃ©rationnelle](#tp2-pt-2--maintien-en-condition-opÃ©rationnelle)
- [Sommaire](#sommaire)
- [0. PrÃ©requis](#0-prÃ©requis)
  - [Checklist](#checklist)
- [I. Monitoring](#i-monitoring)
  - [1. Le concept](#1-le-concept)
  - [2. Setup](#2-setup)
- [II. Backup](#ii-backup)
  - [1. Intwo bwo](#1-intwo-bwo)
  - [2. Partage NFS](#2-partage-nfs)
  - [3. Backup de fichiers](#3-backup-de-fichiers)
  - [4. UnitÃ© de service](#4-unitÃ©-de-service)
    - [A. UnitÃ© de service](#a-unitÃ©-de-service)
    - [B. Timer](#b-timer)
    - [C. Contexte](#c-contexte)
  - [5. Backup de base de donnÃ©es](#5-backup-de-base-de-donnÃ©es)
  - [6. Petit point sur la backup](#6-petit-point-sur-la-backup)
- [III. Reverse Proxy](#iii-reverse-proxy)
  - [1. Introooooo](#1-introooooo)
  - [2. Setup simple](#2-setup-simple)
  - [3. Bonus HTTPS](#3-bonus-https)
- [IV. Firewalling](#iv-firewalling)
  - [1. PrÃ©sentation de la syntaxe](#1-prÃ©sentation-de-la-syntaxe)
  - [2. Mise en place](#2-mise-en-place)
    - [A. Base de donnÃ©es](#a-base-de-donnÃ©es)
    - [B. Serveur Web](#b-serveur-web)
    - [C. Serveur de backup](#c-serveur-de-backup)
    - [D. Reverse Proxy](#d-reverse-proxy)
    - [E. Tableau rÃ©cap](#e-tableau-rÃ©cap)

# 0. PrÃ©requis

âœ [TP2 Part.1](../part1/README.md) terminÃ© : on doit avoir un NextCloud et sa base de donnÃ©es dÃ©diÃ©e

âœ Machines Rocky Linux

âœ Un unique host-only cÃ´tÃ© VBox, Ã§a suffira. **L'adresse du rÃ©seau host-only sera `10.102.1.0/24`.**

âœ Chaque **crÃ©ation de machines** sera indiquÃ© par **l'emoji ğŸ–¥ï¸ suivi du nom de la machine**

âœ Si je veux **un fichier dans le rendu**, il y aura l'**emoji ğŸ“ avec le nom du fichier voulu**. Le fichier devra Ãªtre livrÃ© tel quel dans le dÃ©pÃ´t git, ou dans le corps du rendu Markdown si c'est lisible et correctement formatÃ©.

## Checklist

A chaque machine dÃ©ployÃ©e, vous **DEVREZ** vÃ©rifier la ğŸ“**checklist**ğŸ“ :

- [x] IP locale, statique ou dynamique
- [x] hostname dÃ©fini
- [x] firewall actif, qui ne laisse passer que le strict nÃ©cessaire
- [x] SSH fonctionnel avec un Ã©change de clÃ©
- [x] accÃ¨s Internet (une route par dÃ©faut, une carte NAT c'est trÃ¨s bien)
- [x] rÃ©solution de nom
  - rÃ©solution de noms publics, en ajoutant un DNS public Ã  la machine
  - rÃ©solution des noms du TP, Ã  l'aide du fichier `/etc/hosts`
- [ ] monitoring (oui, toutes les machines devront Ãªtre surveillÃ©es)

# I. Monitoring

On bouge pas pour le moment niveau machines :

| Machine         | IP            | Service                 | Port ouvert | IPs autorisÃ©es |
|-----------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux` | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`  | `10.102.1.12` | Serveur Base de DonnÃ©es | ?           | ?             |

## 1. Le concept

La surveillance ou *monitoring* consiste Ã  surveiller la bonne santÃ© d'une entitÃ©.  
J'utilise volontairement le terme vague "entitÃ©" car cela peut Ãªtre trÃ¨s divers :

- une machine
- une application
- un lien entre deux machines
- etc.

---

**Le monitoring s'effectue en plusieurs Ã©tapes :**

- ***scraping***
  - un programme tourne sur la machine pour rÃ©cupÃ©rer des mÃ©triques sur le systÃ¨me
  - rÃ©cupÃ©rer l'Ã©tat de remplissage de la RAM par exemple
- ***centralisation*** des donnÃ©es (optionnel)
  - dans le cas d'un gros parc de machines, les mÃ©triques rÃ©cupÃ©rÃ©es sont centralisÃ©es sur un unique serveur
- ***visualisation*** des donnÃ©es
  - une joulie interface est dispo pour visualiser les mÃ©triques
  - des courbes dans tous les sens
- ***alerting***
  - l'administrateur dÃ©finit des seuils critiques pour certaines mÃ©triques
  - si le seuil est dÃ©passÃ©, une alerte est envoyÃ©e (mail, Discord, Slack, etc.) pour Ãªtre prÃ©venu immÃ©diatement d'un soucis
  - dans le cas de la RAM par exemple, on sera prÃ©venus **avant** que la RAM soit remplie

---

**Dans notre cas on va surveiller deux choses :**

- d'une part, les machines : ***monitoring systÃ¨me***. Par exemple :
  - remplissage disque/RAM
  - charge CPU/rÃ©seau
- d'autre part, nos applications : ***monitoring applicatif***. Ici :
  - serveur Web
  - base de donnÃ©es

## 2. Setup

De nombreuses solutions de monitoring existent sur le marchÃ©. Nous, on va utiliser [Netdata](https://www.netdata.cloud/).  

Maintenant, vous Ãªtes des techs. Alors la page qui vous intÃ©resse encore plus, c'est [le dÃ©pÃ´t git de la solution](https://github.com/netdata/netdata).

- le README.md y est souvent trÃ¨s complet
- prÃ©sente la solution, et les Ã©tapes d'install
- fournit les liens vers la doc

ğŸŒ **Setup Netdata**

- y'a plein de mÃ©thodes d'install pour Netdata
- on va aller au plus simple, exÃ©cutez, sur toutes les machines que vous souhaitez monitorer :

```bash
# Passez en root pour cette opÃ©ration
$ sudo su -

# Install de Netdata via le script officiel statique
$ bash <(curl -Ss https://my-netdata.io/kickstart-static64.sh)

# Quittez la session de root
$ exit
```

ğŸŒ **Manipulation du *service* Netdata**

- un *service* `netdata` a Ã©tÃ© crÃ©Ã©
- dÃ©terminer s'il est actif, et s'il est paramÃ©trÃ© pour dÃ©marrer au boot de la machine
  - si ce n'est pas le cas, faites en sorte qu'il dÃ©marre au boot de la machine
- dÃ©terminer Ã  l'aide d'une commande `ss` sur quel port Netdata Ã©coute
- autoriser ce port dans le firewall

**Eeeeet.... c'est tout !**, rendez-vous sur `http://IP_VM:PORT` pour accÃ©der Ã  l'interface Web de Netdata (depuis un navigateur sur votre PC).  
**C'est sexy na ? Et c'est en temps rÃ©el :3**

ğŸŒ **Setup Alerting**

- ajustez la conf de Netdata pour mettre en place des alertes Discord
  - *ui ui c'est bien Ã§a :* vous recevrez un message Discord quand un seul critique est atteint
- [c'est lÃ  que Ã§a se passe dans la doc de Netdata](https://learn.netdata.cloud/docs/agent/health/notifications/discord)
- vÃ©rifiez le bon fonctionnement de l'alerting sur Discord

ğŸŒ **Config alerting**

- crÃ©ez une nouvelle alerte pour recevoir une alerte Ã  50% de remplissage de la RAM
- testez que votre alerte fonctionne
  - il faudra remplir artificiellement la RAM pour voir si l'alerte remonte correctement
  - sur Linux, on utilise la commande `stress` pour Ã§a

> Le terme *"stress test"* est employÃ© de faÃ§on gÃ©nÃ©rique pour dÃ©signer le fait de gÃ©nÃ©rer artificiellement de la charge sur un systÃ¨me, afin de l'Ã©prouver, tester comment il rÃ©agit. Vous allez donc ici effectuer un *stress test de la RAM*.

![stress test](./pics/stress-test.jpg)

# II. Backup

| Machine            | IP            | Service                 | Port ouvert | IPs autorisÃ©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de DonnÃ©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |

ğŸ–¥ï¸ **VM `backup.tp2.linux`**

**DÃ©roulez la [ğŸ“**checklist**ğŸ“](#checklist) sur cette VM.**

## 1. Intwo bwo

**La *backup* consiste Ã  extraire des donnÃ©es de leur emplacement original afin de les stocker dans un endroit dÃ©diÃ©.**  

**Cet endroit dÃ©diÃ© est un endroit sÃ»r** : le but est d'assurer la perennitÃ© des donnÃ©es sauvegardÃ©es, tout en maintenant leur niveau de sÃ©curitÃ©.

Pour la sauvegarde, il existe plusieurs faÃ§on de procÃ©der. Pour notre part, nous allons procÃ©der comme suit :

- **crÃ©ation d'un serveur de stockage**
  - il hÃ©bergera les sauvegardes de tout le monde
  - ce sera notre "endroit sÃ»r"
  - ce sera un partage NFS
  - ainsi, toutes les machines qui en ont besoin pourront accÃ©der Ã  un dossier qui leur est dÃ©diÃ© sur ce serveur de stockage, afin d'y stocker leurs sauvegardes
- **dÃ©veloppement d'un script de backup**
  - ce script s'exÃ©cutera en local sur les machines Ã  sauvegarder
  - il s'exÃ©cute Ã  intervalles de temps rÃ©guliers
  - il envoie les donnÃ©es Ã  sauvegarder sur le serveur NFS
  - du point de vue du script, c'est un dossier local. Mais en rÃ©alitÃ©, ce dossier est montÃ© en NFS.

![You're supposed to backup everything](./pics/backup_meme.jpg)

## 2. Partage NFS

ğŸŒ **Setup environnement**

- crÃ©er un dossier `/srv/backup/`
- il contiendra un sous-dossier ppour chaque machine du parc
  - commencez donc par crÃ©er le dossier `/srv/backup/web.tp2.linux/`
- il existera un partage NFS pour chaque machine (principe du moindre privilÃ¨ge)

ğŸŒ **Setup partage NFS**

- je crois que vous commencez Ã  connaÃ®tre la chanson... Google "nfs server rocky linux"
  - [ce lien me semble Ãªtre particuliÃ¨rement simple et concis](https://www.server-world.info/en/note?os=Rocky_Linux_8&p=nfs&f=1)

ğŸŒ **Setup points de montage sur `web.tp2.linux`**

- [sur le mÃªme site, y'a Ã§a](https://www.server-world.info/en/note?os=Rocky_Linux_8&p=nfs&f=2)
- monter le dossier `/srv/backups/web.tp2.linux` du serveur NFS dans le dossier `/srv/backup/` du serveur Web
- vÃ©rifier...
  - avec une commande `mount` que la partition est bien montÃ©e
  - avec une commande `df -h` qu'il reste de la place
  - avec une commande `touch` que vous avez le droit d'Ã©crire dans cette partition
- faites en sorte que cette partition se monte automatiquement grÃ¢ce au fichier `/etc/fstab`

ğŸŒŸ **BONUS** : partitionnement avec LVM

- ajoutez un disque Ã  la VM `backup.tp2.linux`
- utilisez LVM pour crÃ©er une nouvelle partition (5Go Ã§a ira)
- monter automatiquement cette partition au dÃ©marrage du systÃ¨me Ã  l'aide du fichier `/etc/fstab`
- cette nouvelle partition devra Ãªtre montÃ©e sur le dossier `/srv/backup/`

## 3. Backup de fichiers

**Un peu de scripting `bash` !** Le scripting est le meilleur ami de l'admin, vous allez pas y couper hihi.  

La syntaxe de `bash` est TRES particuliÃ¨re, mais ce que je vous demande de rÃ©aliser lÃ  est un script minimaliste.

Votre script **DEVRA**...

- comporter un shebang
- comporter un commentaire en en-tÃªte qui indique le but du script, en quelques mots
- comporter un commentaire qui indique l'auteur et la date d'Ã©criture du script

Par exemple :

```bash
#!/bin/bash
# Simple backup script
# it4 - 09/10/2021

...
```

ğŸŒ **RÃ©diger le script de backup `/srv/tp2_backup.sh`**

- le script crÃ©e une archive compressÃ©e `.tar.gz` du dossier ciblÃ©
  - cela se fait avec la commande `tar`
- l'archive gÃ©nÃ©rÃ©e doit s'appeler `tp2_backup_YYMMDD_HHMMSS.tar.gz`
  - vous remplacerez Ã©videmment `YY` par l'annÃ©e (`21`), `MM` par le mois (`10`), etc.
  - ces infos sont dÃ©terminÃ©es dynamiquement au moment oÃ¹ le script s'exÃ©cute Ã  l'aide de la commande `date`
- le script utilise la commande `rsync` afin d'envoyer la sauvegarde dans le dossier de destination
- il **DOIT** pouvoir Ãªtre appelÃ© de la sorte :

```bash
$ ./tp2_backup.sh <DESTINATION> <DOSSIER_A_BACKUP>
```

ğŸ“ **Fichier `/srv/tp2_backup.sh`**

> **Il est strictement hors de question d'utiliser `sudo` dans le contenu d'un script.**  
Il est envisageable, en revanche, que le script doive Ãªtre lancÃ© avec root ou la commande `sudo` afin d'obtenir des droits Ã©levÃ©s pendant son exÃ©cution.

ğŸŒ **Tester le bon fonctionnement**

- exÃ©cuter le script sur le dossier de votre choix
- prouvez que la backup s'est bien exÃ©cutÃ©e
- **tester de restaurer les donnÃ©es**
  - rÃ©cupÃ©rer l'archive gÃ©nÃ©rÃ©e, et vÃ©rifier son contenu

ğŸŒŸ **BONUS**

- faites en sorte que votre script ne conserve que les 5 backups les plus rÃ©centes aprÃ¨s le `rsync`
- faites en sorte qu'on puisse passer autant de dossier qu'on veut au script : `./tp2_backup.sh <DESTINATION> <DOSSIER1> <DOSSIER2> <DOSSIER3>...` et n'obtenir qu'une seule archive
- utiliser [Borg](https://borgbackup.readthedocs.io/en/stable/) plutÃ´t que `rsync`

## 4. UnitÃ© de service

Lancer le script Ã  la main c'est bien. **Le mettre dans une joulie *unitÃ© de service* et l'exÃ©cuter Ã  intervalles rÃ©guliers, de maniÃ¨re automatisÃ©e, c'est mieux.**

Le but va Ãªtre de crÃ©er un *service* systemd pour que vous puissiez interagir avec votre script de sauvegarde en faisant :

```bash
$ sudo systemctl start tp2_backup
$ sudo systemctl status tp2_backup
```

Ensuite on crÃ©era un *timer systemd* qui permettra de dÃ©clencher le lancement de ce *service* Ã  intervalles rÃ©guliers.

**La classe nan ?**

![systemd can do that](./pics/suprised-cat.jpg)

---

### A. UnitÃ© de service

ğŸŒ **CrÃ©er une *unitÃ© de service*** pour notre backup

- c'est juste un fichier texte hein
- doit se trouver dans le dossier `/etc/systemd/system/`
- doit s'appeler `tp2_backup.service`
- le contenu :

```bash
[Unit]
Description=Our own lil backup service (TP2)

[Service]
ExecStart=/srv/tp2_backup.sh <DESTINATION> <DOSSIER>
Type=oneshot
RemainAfterExit=no

[Install]
WantedBy=multi-user.target
```

> Pour les tests, sauvegardez le dossier de votre choix, peu importe lequel.

ğŸŒ **Tester le bon fonctionnement**

- n'oubliez pas d'exÃ©cuter `sudo systemctl daemon-reload` Ã  chaque ajout/modification d'un *service*
- essayez d'effectuer une sauvegarde avec `sudo systemctl start backup`
- prouvez que la backup s'est bien exÃ©cutÃ©e
  - vÃ©rifiez la prÃ©sence de la nouvelle archive

---

### B. Timer

Un *timer systemd* permet l'exÃ©cution d'un *service* Ã  intervalles rÃ©guliers.

ğŸŒ **CrÃ©er le *timer* associÃ© Ã  notre `tp2_backup.service`**

- toujours juste un fichier texte
- dans le dossier `/etc/systemd/system/` aussi
- fichier `tp2_backup.timer`
- contenu du fichier :

```bash
[Unit]
Description=Periodically run our TP2 backup script
Requires=tp2_backup.service

[Timer]
Unit=tp2_backup.service
OnCalendar=*-*-* *:*:00

[Install]
WantedBy=timers.target
```

> Le nom du *timer* doit Ãªtre rigoureusement identique Ã  celui du *service*. Seule l'extension change : de `.service` Ã  `.timer`. C'est notamment grÃ¢ce au nom identique que systemd sait que ce *timer* correspond Ã  un *service* prÃ©cis.

ğŸŒ **Activez le timer**

- dÃ©marrer le *timer* : `sudo systemctl start tp2_backup.timer`
- activer le au dÃ©marrage avec une autre commande `systemctl`
- prouver que...
  - le *timer* est actif actuellement
  - qu'il est paramÃ©trÃ© pour Ãªtre actif dÃ¨s que le systÃ¨me boot

ğŸŒ **Tests !**

- avec la ligne `OnCalendar=*-*-* *:*:00`, le *timer* dÃ©clenche l'exÃ©cution du *service* toutes les minutes
- vÃ©rifiez que la backup s'exÃ©cute correctement

---

### C. Contexte

ğŸŒ **Faites en sorte que...**

- votre backup s'exÃ©cute sur la machine `web.tp2.linux`
- le dossier sauvegardÃ© est celui qui contient le site NextCloud (quelque part dans `/var/`)
- la destination est le dossier NFS montÃ© depuis le serveur `backup.tp2.linux`
- la sauvegarde s'exÃ©cute tous les jours Ã  03h15 du matin
- prouvez avec la commande `sudo systemctl list-timers` que votre *service* va bien s'exÃ©cuter la prochaine fois qu'il sera 03h15

ğŸ“ **Fichier `/etc/systemd/system/tp2_backup.timer`**  
ğŸ“ **Fichier `/etc/systemd/system/tp2_backup.service`**

## 5. Backup de base de donnÃ©es

Sauvegarder des dossiers c'est bien. Mais sauvegarder aussi les bases de donnÃ©es c'est mieux.

ğŸŒ **CrÃ©ation d'un script `/srv/tp2_backup_db.sh`**

- il utilise la commande `mysqldump` pour rÃ©cupÃ©rer les donnÃ©es de la base de donnÃ©es
- cela gÃ©nÃ¨re un fichier `.sql` qui doit ensuite Ãªtre compressÃ© en `.tar.gz`
- il s'exÃ©cute sur la machine `db.tp2.linux`
- il s'utilise de la faÃ§on suivante :

```bash
$ ./tp2_backup_db.sh <DESTINATION> <DATABASE>
```

ğŸ“ **Fichier `/srv/tp2_backup_db.sh`**  

ğŸŒ **Restauration**

- tester la restauration de donnÃ©es
- c'est Ã  dire, une fois la sauvegarde effectuÃ©e, et le `tar.gz` en votre possession, tester que vous Ãªtes capables de restaurer la base dans l'Ã©tat au moment de la sauvegarde
  - il faut rÃ©injecter le fichier `.sql` dans la base Ã  l'aide d'une commmande `mysql`

ğŸŒ ***UnitÃ© de service***

- pareil que pour la sauvegarde des fichiers ! On va faire de ce script une *unitÃ© de service*.
- votre script `/srv/tp2_backup_db.sh` doit pouvoir se lancer grÃ¢ce Ã  un *service* `tp2_backup_db.service`
- le *service* est exÃ©cutÃ© tous les jours Ã  03h30 grÃ¢ce au *timer* `tp2_backup_db.timer`
- prouvez le bon fonctionnement du *service* ET du *timer*

ğŸ“ **Fichier `/etc/systemd/system/tp2_backup_db.timer`**  
ğŸ“ **Fichier `/etc/systemd/system/tp2_backup_db.service`**

## 6. Petit point sur la backup

A ce stade vous avez :

- un script qui tourne sur `web.tp2.linux` et qui **sauvegarde les fichiers de NextCloud**
- un script qui tourne sur `db.tp2.linux` et qui **sauvegarde la base de donnÃ©es de NextCloud**
- toutes **les backups sont centralisÃ©es** sur `backup.tp2.linux`
- **tout est gÃ©rÃ© de faÃ§on automatisÃ©e**
  - les scripts sont packagÃ©s dans des *services*
  - les services sont dÃ©clenchÃ©s par des *timers*
  - tout est paramÃ©trÃ© pour s'allumer quand les machines boot (les *timers* comme le serveur NFS)

ğŸ”¥ğŸ”¥ **That is clean shit.** ğŸ”¥ğŸ”¥

# III. Reverse Proxy

## 1. Introooooo

Un *reverse proxy* est un outil qui sert d'intermÃ©diaire entre le client et un serveur donnÃ© (souvent un serveur Web).

**C'est l'admin qui le met en place, afin de protÃ©ger l'accÃ¨s au serveur Web.**

Une fois en place, le client devra saisir l'IP (ou le nom) du *reverse proxy* pour accÃ©der Ã  l'application Web (ce ne sera plus directement l'IP du serveur Web).

Un *reverse proxy* peut permettre plusieurs choses :

- chiffrement
  - c'est lui qui mettra le HTTPS en place (protocole HTTP + chiffrement avec le protocole TLS)
  - on pourrait le faire directement avec le serveur Web (Apache) dans notre cas
  - pour de meilleures performances, il est prÃ©fÃ©rable de dÃ©dier une machine au chiffrement HTTPS, et de laisser au serveur web un unique job : traiter les requÃªtes HTTP
- rÃ©partition de charge
  - plutÃ´t qu'avoir un seul serveur Web, on peut en setup plusieurs
  - ils hÃ©bergent tous la mÃªme application
  - le *reverse proxy* enverra les clients sur l'un ou l'autre des serveurs Web, afin de rÃ©partir la charge Ã  traiter
- d'autres trucs
  - caching de ressources statiques (CSS, JSS, images, etc.)
  - tolÃ©rance de pannes
  - ...

---

**Dans ce TP on va setup un reverse proxy NGINX trÃ¨s simpliste.**

![Apache at the back hihi](./pics/nginx-at-the-front-apache-at-the-back.jpg)

## 2. Setup simple

| Machine            | IP            | Service                 | Port ouvert | IPs autorisÃ©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de DonnÃ©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |
| `front.tp2.linux`  | `10.102.1.14` | Reverse Proxy           | ?           | ?             |

ğŸ–¥ï¸ **VM `front.tp2.linu`x**

**DÃ©roulez la [ğŸ“**checklist**ğŸ“](#checklist) sur cette VM.**

ğŸŒ **Installer NGINX**

- vous devrez d'abord installer le paquet `epel-release` avant d'installer `nginx`
  - EPEL c'est des dÃ©pÃ´ts additionnels pour Rocky
  - NGINX n'est pas prÃ©sent dans les dÃ©pÃ´ts par dÃ©faut que connaÃ®t Rocky
- le fichier de conf principal de NGINX est `/etc/nginx/nginx.conf`

ğŸŒ **Tester !**

- lancer le *service* `nginx`
- le paramÃ©trer pour qu'il dÃ©marre seul quand le systÃ¨me boot
- repÃ©rer le port qu'utilise NGINX par dÃ©faut, pour l'ouvrir dans le firewall
- vÃ©rifier que vous pouvez joindre NGINX avec une commande `curl` depuis votre PC

ğŸŒ **Explorer la conf par dÃ©faut de NGINX**

- repÃ©rez l'utilisateur qu'utilise NGINX par dÃ©faut
- dans la conf NGINX, on utilise le mot-clÃ© `server` pour ajouter un nouveau site
  - repÃ©rez le bloc `server {}` dans le fichier de conf principal
- par dÃ©faut, le fichier de conf principal inclut d'autres fichiers de conf
  - mettez en Ã©vidence ces lignes d'inclusion dans le fichier de conf principal

ğŸŒ **Modifier la conf de NGINX**

- pour que Ã§a fonctionne, le fichier `/etc/hosts` de la machine **DOIT** Ãªtre rempli correctement, conformÃ©ment Ã  la **[ğŸ“**checklist**ğŸ“](#checklist)**
- supprimer le bloc `server {}` par dÃ©faut, pour ne plus prÃ©senter la page d'accueil NGINX
- crÃ©er un fichier `/etc/nginx/conf.d/web.tp2.linux.conf` avec le contenu suivant :
  - j'ai sur-commentÃ© pour vous expliquer les lignes, n'hÃ©sitez pas Ã  dÃ©gommer mes lignes de commentaires

```bash
[it4@localhost nginx]$ cat conf.d/web.tp2.linux.conf 
server {
    # on demande Ã  NGINX d'Ã©couter sur le port 80 pour notre NextCloud
    listen 80;

    # ici, c'est le nom de domaine utilisÃ© pour joindre l'application
    # ce n'est pas le nom du reverse proxy, mais le nom que les clients devront saisir pour atteindre le site
    server_name web.tp2.linux; # ici, c'est le nom de domaine utilisÃ© pour joindre l'application (pas forcÃ©me

    # on dÃ©finit un comportement quand la personne visite la racine du site (http://web.tp2.linux/)
    location / {
        # on renvoie tout le trafic vers la machine web.tp2.linux
        proxy_pass http://web.tp2.linux;
    }
}
```

## 3. Bonus HTTPS

**Etape bonus** : mettre en place du chiffrement pour que nos clients accÃ¨dent au site de faÃ§on plus sÃ©curisÃ©e.

ğŸŒŸ **GÃ©nÃ©rer la clÃ© et le certificat pour le chiffrement**

- il existe plein de faÃ§ons de faire
- nous allons gÃ©nÃ©rer en une commande la clÃ© et le certificat
- puis placer la clÃ© et le cert dans les endroits standards pour la distribution Rocky Linux

```bash
# On se dÃ©place dans un dossier oÃ¹ on peut Ã©crire
$ cd ~

# GÃ©nÃ©ration de la clÃ© et du certificat
# Attention Ã  bien saisir le nom du site pour le "Common Name"
$ openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout server.key -out server.crt
[...]
Common Name (eg, your name or your server\'s hostname) []:web.tp2.linux
[...]

# On dÃ©place la clÃ© et le certificat dans les dossiers standards sur Rocky
# En le renommant
$ sudo mv server.key /etc/pki/tls/private/web.tp2.linux.key
$ sudo mv server.crt /etc/pki/tls/certs/web.tp2.linux.crt

# Setup des permissions restrictives
$ sudo chown root:root /etc/pki/tls/private/web.tp2.linux.key
$ sudo chown root:root /etc/pki/tls/certs/web.tp2.linux.crt
$ sudo chmod 400 /etc/pki/tls/private/web.tp2.linux.key
$ sudo chmod 644 /etc/pki/tls/certs/web.tp2.linux.crt
```

ğŸŒŸ **Modifier la conf de NGINX**

- inspirez-vous de ce que vous trouvez sur internet
- il n'y a que deux lignes Ã  ajouter
  - une ligne pour prÃ©ciser le chemin du certificat
  - une ligne pour prÃ©ciser le chemin de la clÃ©
- et une ligne Ã  modifier
  - prÃ©ciser qu'on Ã©coute sur le port 443, avec du chiffrement
- n'oubliez pas d'ouvrir le port 443/tcp dans le firewall

ğŸŒŸ **TEST**

- connectez-vous sur `https://web.tp2.linux` depuis votre PC
- petite avertissement de sÃ©cu : normal, on a signÃ© nous-mÃªmes le certificat
  - vous pouvez donc "Accepter le risque" (le nom du bouton va changer suivant votre navigateur)
  - avec `curl` il faut ajouter l'option `-k` pour dÃ©sactiver cette vÃ©rification

# IV. Firewalling

**On va rendre nos firewalls un peu plus agressifs.**

Actuellement je vous ai juste demandÃ© d'autoriser le trafic sur tel ou tel port. C'est bien.

**Maintenant on va restreindre le trafic niveau IP aussi.**

Par exemple : notre base de donnÃ©es `db.tp2.linux` n'est accÃ©dÃ©e que par le serveur Web `web.tp2.linux`, et par aucune autre machine.  
On va donc configurer le firewall de la base de donnÃ©es pour qu'elle n'accepte QUE le trafic qui vient du serveur Web.

**On va *harden* ("durcir" en franÃ§ais) la configuration de nos firewalls.**

## 1. PrÃ©sentation de la syntaxe

> **N'oubliez pas d'ajouter `--permanent` sur toutes les commandes `firewall-cmd`** si vous souhaitez que le changement reste effectif aprÃ¨s un rechargement de FirewallD.

**PremiÃ¨re Ã©tape** : dÃ©finir comme politique par dÃ©faut de TOUT DROP. On refuse tout, et on whiteliste aprÃ¨s.

Il existe dÃ©jÃ  une zone appelÃ©e `drop` qui permet de jeter tous les paquets. Il suffit d'ajouter nos interfaces dans cette zone.

```bash
$ sudo firewall-cmd --list-all # on voit qu'on est par dÃ©faut dans la zone "public"
$ sudo firewall-cmd --set-default-zone=drop # on configure la zone "drop" comme zone par dÃ©faut
$ sudo firewall-cmd --zone=drop --add-interface=enp0s8 # ajout explicite de l'interface host-only Ã  la zone "drop"
```

**Ensuite**, on peut crÃ©er une nouvelle zone, qui autorisera le trafic liÃ© Ã  telle ou telle IP source :

```bash
$ sudo firewall-cmd --add-zone=ssh # le nom "ssh" est complÃ¨tement arbitraire. C'est clean de faire une zone par service.
```

**Puis** on dÃ©finit les rÃ¨gles visant Ã  autoriser un trafic donnÃ© :

```bash
$ sudo firewall-cmd --zone=ssh --add-source=10.102.1.1/32 # 10.102.1.1 sera l'IP autorisÃ©e
$ sudo firewall-cmd --zone=ssh --add-port=22/tcp # uniquement le trafic qui vient 10.102.1.1, Ã  destination du port 22/tcp, sera autorisÃ©
```

**Le comportement de FirewallD sera alors le suivant :**

- si l'IP source d'un paquet est `10.102.1.1`, il traitera le paquet comme Ã©tant dans la zone `ssh`
- si l'IP source est une autre IP, et que le paquet arrive par l'interface `enp0s8` alors le paquet sera gÃ©rÃ© par la zone `drop` (le paquet sera donc *dropped* et ne sera jamais traitÃ©)

> *L'utilisation de la notation `IP/32` permet de cibler une IP spÃ©cifique. Si on met le vrai masque `10.102.1.1/24` par exemple, on autorise TOUT le rÃ©seau `10.102.1.0/24`, et non pas un seul hÃ´te. Ce `/32` c'est un truc qu'on voit souvent en rÃ©seau, pour faire rÃ©fÃ©rence Ã  une IP unique.*

![Cut here to activate firewall :D](./pics/cut-here-to-activate-firewall-best-label-for-lan-cable.jpg)

## 2. Mise en place

### A. Base de donnÃ©es

ğŸŒ **Restreindre l'accÃ¨s Ã  la base de donnÃ©es `db.tp2.linux`**

- seul le serveur Web doit pouvoir joindre la base de donnÃ©es sur le port 3306/tcp
- vous devez aussi autoriser votre accÃ¨s SSH
- n'hÃ©sitez pas Ã  multiplier les zones (une zone `ssh` et une zone `db` par exemple)

> Quand vous faites une connexion SSH, vous la faites sur l'interface Host-Only des VMs. Cette interface est branchÃ©e Ã  un Switch qui porte le nom du Host-Only. Pour rappel, votre PC a aussi une interface branchÃ©e Ã  ce Switch Host-Only.  
C'est depuis cette IP que la VM voit votre connexion. C'est cette IP que vous devez autoriser dans le firewall de votre VM pour SSH.

ğŸŒ **Montrez le rÃ©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

- `sudo firewall-cmd --get-active-zones`
- `sudo firewall-cmd --get-default-zone`
- `sudo firewall-cmd --list-all --zone=?`

### B. Serveur Web

ğŸŒ **Restreindre l'accÃ¨s au serveur Web `web.tp2.linux`**

- seul le reverse proxy `front.tp2.linux` doit accÃ©der au serveur web sur le port 80
- n'oubliez pas votre accÃ¨s SSH

ğŸŒ **Montrez le rÃ©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### C. Serveur de backup

ğŸŒ **Restreindre l'accÃ¨s au serveur de backup `backup.tp2.linux`**

- seules les machines qui effectuent des backups doivent Ãªtre autorisÃ©es Ã  contacter le serveur de backup *via* NFS
- n'oubliez pas votre accÃ¨s SSH

ğŸŒ **Montrez le rÃ©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### D. Reverse Proxy

ğŸŒ **Restreindre l'accÃ¨s au reverse proxy `front.tp2.linux`**

- seules les machines du rÃ©seau `10.102.1.0/24` doivent pouvoir joindre le proxy
- n'oubliez pas votre accÃ¨s SSH

ğŸŒ **Montrez le rÃ©sultat de votre conf avec une ou plusieurs commandes `firewall-cmd`**

### E. Tableau rÃ©cap

ğŸŒ **Rendez-moi le tableau suivant, correctement rempli :**

| Machine            | IP            | Service                 | Port ouvert | IPs autorisÃ©es |
|--------------------|---------------|-------------------------|-------------|---------------|
| `web.tp2.linux`    | `10.102.1.11` | Serveur Web             | ?           | ?             |
| `db.tp2.linux`     | `10.102.1.12` | Serveur Base de DonnÃ©es | ?           | ?             |
| `backup.tp2.linux` | `10.102.1.13` | Serveur de Backup (NFS) | ?           | ?             |
| `front.tp2.linux`  | `10.102.1.14` | Reverse Proxy           | ?           | ?             |
